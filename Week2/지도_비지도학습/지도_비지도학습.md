# 지도_비지도학습

## 지도학습과 비지도학습

항목|지도학습|비지도학습
-|-|-
목표|정답 예측|숨겨진 패턴 탐색
데이터|입력과 정답(label)|정답 없이 입력만 존재
예시|스팸 메일 분류, 집값 예측|고객 군집화, 이상치 탐지
출력|명확한 예측 결과|패턴, 군집 등의 추론
주요 알고리즘|회귀, 분류, 결정트리, SVM 등|K-means, DBSCAN, PCA 등

---

## 지도학습

입력 데이터와 정답(label)이 쌍으로 주어진 데이터를 통해 학습함

목표 : 주어진 데이터로 학습하여 처음 보는 데이터의 정답을 예측하는 모델을 만드는 것

### 1. 회귀(Regression)

연속적인 수치를 예측하는 문제

지표 : MSE, RMSE 등

<손실함수>

**모델의 예측값이 실제값과 얼마나 다른지를 수치로 측정**

- **MSE(Mean Squared Error)**

  $$MSE = \frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\hat{y_i})^2$$

  오차의 제곱에 대한 평균

  제곱 수행 $\rightarrow$ 큰 오차에 더욱 민감함

- **RMSE(Root Mean Squared Error)**

  $$RMSE = \sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\hat{y_i})^2}$$

  MSE에 루트 계산 $\rightarrow$ 실제 단위와 동일해지 때문에 해석이 더 직관적임


### 2. 분류(Classification)

정해진 카테고리를 예측하는 문제

지표 : Accuracy, Precision 등

<손실함수>

**모델이 얼마나 정확하게 맞췄는가(분류했는가)를 평가**

Real Label <br> ----- <br> Predicted Label|Positive|Negative
-|-|-
**Positive**|True Positive <br> (TP)|False Positive <br> (FP)
**Negative**|False Negative <br> (FN)|True Negative <br> (TN)

- **Accuracy(정확도)**

  $$\frac{TP+TN}{TP+FP+FN+TN}$$

  전체 중에서 맞춘 비율

- **Precisiion(정밀도)**

  $$\frac{TP}{TP+FP}$$

  참이라고 예측한 것 중에서 실제로 참인 비율

- **Recall(재현율)**

  $$\frac{TP}{TP+FN}$$

  실제로 참인 것 중에서 맞춘 비율

### 선형 회귀(Linear Regression)

데이터를 직선으로 근사하여 예측하는 대표적인 지도학습 알고리즘

$\hat y = wX + b$

- 입력 변수 X에 대해 직선을 학습해서 출력 y 예측

- 목적 : 실제값과 예측값의 오차(차이)를 최소화 = MSE(평균 제곱 오차)를 최소화

  * $w$ : 가중치(weight) - feature가 target 예측에 미치는 영향 정도
 
  * $b$ : 편향(bias) - 모든 feature가 0일 때 target 값, 항상 일정한 방향으로 치우친 예측 방지
 
#### 과적합과 규제(Regularization)

- **과적합(Overfitting)** : 훈련 데이터는 완벽히 반영하지만 테스트에서는 성능이 저하되는 현상

- **규제(Regularization)** : 모델의 복잡도를 줄이는 방법, 과적합의 대표적인 해결방안

  $\hookrightarrow$ 규제항을 추가하여 의도적으로 손실 함수를 키워 가중치 크기를 크지 않도록 제어하여 과적합 방지

  - **L2 정규화(Ridge Regression)**
 
    $$\lambda\sum w_j^2$$

    $\lambda$ : 정규화 강도를 조절하는 하이퍼 파라미터

    모든 가중치를 조금씩 작게 만듬 $\rightarrow$ 가중치가 클수록 더 큰 패널티 부여 $\rightarrow$ 점점 0에 가까워지나 0이 되지는 않음

  - **L1 정규화(Lasso Regression)**
 
    $$\lambda\sum|w_j|$$

    $\lambda$ : 정규화 강도를 조절하는 하이퍼 파라미터

    일부 가중치를 정확히 0으로 만듬 $\rightarrow$ 특성 선택 효과가 있어 불필요한 특성을 자동으로 제거하여 해석이 쉬움

### 로지스틱 회귀(Logistic Regression)

이진 분류(Binary Classification)를 수행하는 선형 모델

선형 회귀로 게산된 값을 시그모이드 함수에 넣어 확률값(0~1)으로 변환하고 이를 기준으로 클래스 분류

$y = \sigma(w^Tx+b) = \frac{1}{1+e^{-(w^tx+b)}}$

예측값 y는 0~1 사이의 확률을 나타내며 기준값을 넘으면 1, 아니면 0으로 분류

손실 함수 : Cross Entropy $\rightarrow$ 확률 기반 예측의 오차를 측정하며 예측이 정답에서 멀수록 큰 손실 발생

### Decision Tree(의사 결정 트리)

입력 데이터의 특성(feature)을 기준으로 조건 분기를 반복하면서 데이터를 여러 그룹으로 나누는 분류/회귀 모델

- 동작 방식 : 모든 feature 중 데이터를 가장 잘 나눌 수 있는 기준 선택 $\rightarrow$ 해당 기준으로 데이터 분할 $/rightarrow$ 분할을 반복하다 정지 조건을 만족하면 분할 중단 후 리프 노드 설정

- 분할 기준

  - 분류 : 지니 불순도, 엔트로피 (분할 전후의 불순도를 계산하여 불순도가 가장 많이 감소하는 분할 선택)

      - **Gini Index**

        데이터를 무작위로 선택했을 때 잘못 분류될 확률

        클래스가 하나로 완벽하게 분리된 노드에서는 Gini 지수가 0 = 불순도가 없음

        값이 클수록 클래스가 섞여 있음을 의미
        
        $$Gini = 1 - \sum p_i^2$$
      
       - **Entropy**

         데이터의 혼잡도(불확실성)를 측정
         
         클래스 분포가 균등할수록 값이 커지고 하나의 클래스만 존재하면 0이 됨

         $$Entropy = - \displaystyle\sum_i p_i log_2(p_i)$$
         
  - 회귀 : 분산, MSE

트리를 깊게 만들면 **과적합** 발생 $\rightarrow$ 최대 깊이를 제한하거나 최소 샘플 수 제한 등을 통해 해결 가능

### SVM(Support Vector Machine)

클래스 간의 경계를 가장 잘 구분하는 직선(혹은 초평면)을 찾는 분류 알고리즘

- 목표 : 최대 margin을 만드는 경계를 찾는 것, support vector가 경계를 결정함

  - 결정 경계 : 데이터를 가장 잘 분류하는 가장 적절한 초평면(Decision Boundary)
 
  - Margin 최대화 : 초평면과 각 클래스의 데이터 사이 거리를 최대화하여 일반화 성능을 높임
 
    - **Hard Margin**

      outlier들을 무시하지 않고 support vector를 찾으며 결정 경계를 침범하지 않도록 함

      모든 데이터가 초평면을 기준으로 완벽하게 분리 가능하다고 가정하여 진행
 
      margin이 매우 좁아질 수 있으며 이상치 등을 억지로 분류하면 경계가 극단적으로 학습되어 과적합 발생 가능

    - **Soft Margin**
   
      일부 오차를 허용하되 전체적인 마진을 최대화하는 것이 목표

      하이퍼 파라미터를 통해 오차 허용 정도 조절 가능 (오차 허용 $\downarrow$ $\rightarrow$ 마진 좁음 / 오차 허용 $\uparrow$ $\rightarrow$ 마진 넒음)

  - Support Vector : 초평면과 가장 가까운 데이터 포인트, margin을 결정하는 중요한 역할 수행

 ### 커널 트릭(Kernel Trick)

입력 데이터를 고차원 공간으로 맵핑하여 고차원에서는 선형 분리가 가능하게 만드는 것

현실 데이터는 대부분 2차원 평면에서 선형 분리 불가능하다는 한계점을 해결하기 위해 사용되는 기법

커널 함수를 이용하여 데이터 간 내적 연산만으로 고차원 공간 효과를 누릴 수 있음 $\rightarrow$ 실제 고차원 공간으로 보내는 것이 아닌 고차원에서 계산한 것처럼 결과를 얻을 수 있는 기법

ex) RBF, Linear Kernel, Polynominal Kernel 등

### SVR(Support Vector Regression)

SVM을 분류가 아닌 회귀 문제에 적용하기 위해 사용되는 기법

단순한 선형 회귀 형태가 아닌 마진 안쪽에 최대한 많은 점을 넣으려고 함

$\hookrightarrow$ 마진 안에 들어가는 예측 오차는 허용하지만 마진 밖에 있는 예측 오차에는 패널티를 부여함

기존 선형 회귀에 비해 모델이 덜 민감하고 일반화 능력이 좋음

---------

## 비지도학습

등장 배경 : 실제 많은 데이터는 Label이 없는 경우가 훨씬 많은데 라벨링 작업은 비용이 크고 시간이 많이 걸림

정답(Label) 없이 입력 데이터만 주어진 상황에서 새로운 규칙성을 알아내는 학습 방식

목표 : 여러 문제를 학습하여 해당 데이터의 패턴, 구조, 군집, 이상치 등을 스스로 파악

### 군집화(Clustering)

정답이 없는 데이터에서 유사한 데이터끼리 그룹화

거리, 밀도, 분포, 통계적 유사성 등을 기준으로 군집 생성

목표 : 군집 내부는 유사하게 군집 간은 이질적이게 만들기

알고리즘|핵심 아이디어|군집 수 설정|장점|단점
-|-|-|-|-
K-Means|중심점 기준 거리|필요|빠르고 단순함|초기값에 민감, 구형 데이터에 적합
Hierarchical|트리 구조의 분할과 병합|불필요|구조 해석 쉬움|속도가 느리고 변화에 민감
DBSCAN|밀도 기반|불필요|이상치 탐지|파라미터에 민감
GMM|확률적 소속|필요|유연한 분포|계산 복잡

- **K-Means Clustering**

  군집 수 k를 미리 정하고 각 군집의 중심(centroid)을 기준으로 데이터를 그룹화하는 기법

  직관적이고 빠름, 군집 모양이 둥글고 균일할 때 성능이 좋음, outlier에 민감, 초기에 정하는 centroid 값에 민감함

  과정 : 1. k개의 중심점(centroid) 정하기 $\rightarrow$ 2. 각 데이터를 가장 가까운 중심에 할당 $\rightarrow$ 3. 중심점 이동(각 군집에 속한 데이터들의 평균을 계산해 새로운 중심점 설정) $\rightarrow$ 4. 수렴 조건 확인(중심점이 더 이상 크게 이동하지 않으면 반복을 멈춤, 그렇지 않으면 다시 2단계로 이동)

- **Hierarchical Clustering(계층적 군집화)**

  계층적 군집화는 트리 구조를 통해 데이터들을 유사도에 따라 계층적으로 묶어가는 군집화 방식

  가장 가까운 거리에 있는 데이터를 서로 묶는 과정을 반복 수행하여 최종으로 하나의 군집으로 합쳐질 때까지 진행

  결과는 덴드로그램(Dendrogram) 형태로 표현, 자르는 높이에 따라 군집 수 조절 가능

  과정 : 1. 모든 데이터를 개별 군집으로 시작 $\rightarrow$ 2. 유사도가 가장 가까운 두 군집 선택 (Linkage 방식에 따라 계산 상이 - Single : 두 군집 사이에서 가장 가까운 두 점 사이의 거리, Complete : 두 군집 사이에서 가장 먼 두 점 사이의 거리, Average : 두 군집 간 모든 점의 거리 평균)$\rightarrow$ 3. 두 군집 병합
  
- **DBSCAN**

  데이터의 밀도를 기준으로 군집을 형성하고 밀도가 낮은 점은 노이즈로 간주하는 알고리즘

  하이퍼파라미터 : Epsilon - 한 점 주변을 탐색할 반경(반지름), Min Points - 군집 구성 시 필요한 최소 데이터 포인트 수(밀도 기준)

  군집 수를 미리 지정할 필요 없음, 복잡한 형태의 군집도 탐지 가능, 이상치 탐지 가능, 주요 파라미터 값에 민감하며 예측과 해석이 어려움

  과정 : 1. 임의의 데이터 포인트 선택 $\rightarrow$ 2. 선택한 포인트의 반지름 $\epsilon$ 내 포인트 찾기 $\rightarrow$ 해당 영역 내에 포인트 수가 MinPts 이상이면 새로운 군집 생성 $\rightarrow$ 4. 새로 포함된 포인트 중에서 Core Point 조건을 만족하는 포인트가 있으면 그 주변도 확장하며 군집 키움 $\rightarrow$ 5. 모든 포인트에 대해 과정 반복 $\rightarrow$ 6. 어떤 군집에도 포함되지 못한 포인트는 노이즈로 간주
  
- **GMM**

  데이터가 여러 개의 정규분포(가우시안 분포)를 따르고 있다고 가정하여 각 데이터가 군집에 속할 확률을 계산하는 군집화 알고리즘

  각 군집은 하나의 가우시안 분포로 모델링됨

  하나의 데이터 포인터는 여러 군집에 속할 수 있으며 각 군집에 속할 확률 값이 있음

  학습 방식 : EM 알고리즘(Expectation - Maximization)

    E-step : 각 데이터가 각 군집에 속할 확률 계산

    M-step : 그 확률을 기반으로 가우시안 파라미터 갱신
