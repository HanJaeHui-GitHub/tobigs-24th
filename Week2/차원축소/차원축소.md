# 차원 축소 (Dimensionality Reduction)

## 차원

공간 내에 있는 점의 위치를 나타내기 위해 필요한 축의 개수

데이터 분석 차원 : 점 = 데이터, 축 = Feature(특성)

차원의 크기가 커질수록 데이터를 해석할 수 있는 정보를 더 많이 가짐

### 차원의 저주 (Curse of Dimensionality)

데이터 차원이 증가할수록 데이터를 담고 있는 공간의 부피가 기하급수적으로 증가

$\rightarrow$ 데이터 밀도가 희소해져 데이터 간 거리가 멀어짐 

$\rightarrow$ 모델의 공간 대부분이 비어있는 상태

- 문제점

  비효율성: 데이터 밀도가 낮아 빈 공간이 많아져 분석에 과도한 부담을 주어 성능을 느리게 하거나 분석을 어렵게 함
  
  과적합: 많은 변수로 인해 모델의 복잡도가 증가하여 과적합 위험성이 커짐

  설명력: 차원이 높아질수록 데이터의 특성이나 분석 모델을 이해하고 설명하기 힘들어짐.

목표 : 데이터에 담긴 정보를 최대한 유지하면서, 불필요한 중복 정보나 노이즈는 감소

## EVD/ SVD

### 고유벡터(Eigenvalue)

선형변환을 적용해도 방향이 바뀌지 않는 벡터

선형 변환의 축

### 고유값(Eigenvector)

고유벡터의 방향으로 나타난 크기 변화 비율 = 축 방향의 크기 변화율

### EVD (Eigen-Value Decomposition - 고유값 분해)**

행렬을 고유벡터 기준으로 좌표계를 바꿔서 그 공간에서 단순한 스케일링을 적용한 뒤 다시 원래 공간으로 되돌리는 방식

과정 : 1. 고유벡터 기준으로 좌표계 돌리기 $\rightarrow$ 2. 고유값만큼 늘리기 $\rightarrow$ 3. 다시 원래 좌표계로 돌리기

$\hookrightarrow$ 복잡한 선형 변환도 고유벡터라는 기준으로 보면 단순히 스케일 조정만 한 것처럼 해석 가능

조건 : 행렬 A는 nxn 정방행렬, n개의 선형 독립 고유벡터(= 대각화 가능)를 가져야 함

### SVD (Singular-Value Decomposition - 특이값 분해)

모든 형태의 행렬에 적용 가능한 분해 기법

EVD와 유사하게 행렬 U와 V^T로 좌표계를 돌리고, 특이값으로 이루어진 Sigma 행렬로 크기를 늘리는 "돌리고-늘리고-돌리고" 변환을 통해 모든 행렬을 분해 가능

## PCA / LDA

### 주성분 분석 (PCA - Principal Component Analysis)

기존 데이터에서 분산을 최대로 보존할 수 있는 새로운 축(= 주성분, principal component, PC)을 찾아 데이터를 투영하여 차원을 축소

분산이 크다 = 데이터 사이의 차이점이 명확함 = 중요한 정보를 많이 담고 있다는 의미 $\rightarrow$ 분산을 최대로 보존하는 이유

과정
1.  가장 큰 분산을 가지는 첫 번째 주성분 축(고유벡터 v1)을 생성
2.  첫 번째 축에 직각이 되는 방향 중에서 다음으로 분산이 큰 두 번째 주성분 축(고유벡터 v2)을 선택
3.  각 주성분 축은 서로 직교하도록 하여, 중복된 정보 없이 독립적인 방향으로 데이터를 압축
4.  원하는 차원 수만큼 이 과정을 반복하여 주성분 축을 생성

입력 데이터의 **공분산 행렬의 EVD나 SVD**를 통해 주성분(고유벡터)을 구할 수 있음.

### 선형 판별 분석법 (LDA - Linear Discriminant Analysis)

클래스 분리를 최대화하는 축을 찾기 위해, 클래스 내 분산은 최소, 클래스 간 거리는 최대가 되게 하는 벡터를 찾아 데이터를 투영하여 차원 축소

- PCA VS LDA 비교

  - 공통점: 데이터셋의 차원 개수를 줄이는 선형 변환 기법

  - 차이점

    학습 방식 : PCA - 비지도 학습 기법, LDA - 지도 학습 기법

    목표 : PCA - 데이터의 레이블 정보 없이 데이터의 분산을 기준으로 주성분 축 설정(데이터 전체 분산 보존), LDA - 클래스 레이블 정보를 활용하여 클래스 간 분산은 최대화하고 클래스 내 분산은 최소화

    사용 행렬 : PCA- 공분산 행렬, LDA - 클래스 내부 분산 행렬과 클래스 간 분산 행렬

## t-SNE / UMAP

고차원 데이터를 저차원 공간으로 시각화하기 위해 사용되는 비선형 차원 축소 기법

### t-SNE (t-Distributed Stochastic Neighbor Embedding, t-분포 확률적 이웃 임베딩)

고차원 공간에서 비슷한 데이터 구조는 저차원 공간에서 가깝게, 비슷하지 않은 구조는 멀리 떨어져 대응되도록 시각화함

작동 원리
1.  고차원 데이터에서의 유사성 계산 : 데이터 포인트 i가 데이터 포인트 j를 선택할 조건부 확률을 **가우시안 분포**를 기반으로 계산
2.  저차원 데이터에서의 유사성 계산 : 고차원과 동일한 조건부 확률을 **t-분포**를 기반으로 계산(t-분포는 꼬리가 길고 두꺼워 멀리 떨어진 점들 간의 거리 차이를 잘 반영하며, 클러스터 간 간격을 더 뚜렷하게 분리함)
3.  KL Divergence 최소화 : 고차원 공간과 저차원 공간의 유사성 확률 분포 사이의 차이인 **KL 발산**을 최소화하는 것이 t-SNE의 목표
4.  확률적 경사 하강법(SGD) 최적화 : KL Divergence를 최소화하는 방향으로 저차원 좌표를 반복적으로 갱신

- 장점

  비선형 변환 가능 $\rightarrow$ PCA와 달리 복잡한 데이터 구조 시각화 가능

  데이터 간 겹침을 완화하고 분리를 강조

  하이퍼파라미터 영향이 적고 이상치에 둔감

  클러스터 구조 표현에 강점을 가짐
  
- 단점

  연산량이 많아 데이터 수가 증가하면 시간 오래 걸림

  초고차원 데이터를 바로 축소하기 어렵고, 보통 50차원 이내로 사전 축소 후 적용하는 것이 일반적

  결과가 비결정적 $\rightarrow$ 매 실행 시 시각화 결과가 달라질 수 있음

  저차원 임베딩 시 정보 손실이 발생하여 데이터 왜곡 우려가 있음
  
### UMAP (Uniform Manifold Approximation and Projection, 균일한 매니폴드 근사 및 투영)

t-SNE의 연산량 증가 문제와 전역 구조 왜곡 문제를 보완하기 위해 개발됨

작동 원리 : 고차원 공간에서 데이터 간의 이웃 관계를 그래프 형태로 나타낸 뒤, 저차원에서도 이 구조를 최대한 보존할 수 있도록 확률적 최적화를 통해 좌표를 조정하는 방식

- 파라미터

  n_neighbors : 초기 고차원 그래프 생성 시 사용되는 이웃의 숫자, 값이 클수록 데이터의 전역(global) 구조를 더 정확하게 이끌어 냄, 작을수록 지역(local) 구조에 집중

  min_dist : 저차원 공간에서 포인트 간의 최소 거리, 값이 커지면 UMAP에 사영된 포인트들을 분산시켜 데이터 클러스터링 약화, 전역 구조를 덜 강조함

- 장점

  t-SNE보다 훨씬 빠름

  결과 재현성이 높음 $\rightarrow$ 반복 실험이 필요한 대규모 데이터 분석에 효과적

  데이터의 전역 구조를 더 잘 보존

- 해석 시 주의할 점

  하이퍼파라미터가 결과에 큰 영향을 미침 $\rightarrow$ 다양한 파라미터 조합으로 실험 필요

  클러스터의 상대적인 크기는 특별한 의미가 없음 (크다고 중요하거나 크다는 뜻이 아님)

  클러스터 간 거리 해석에 주의해야 함 (전역 구조는 보존되나, 클러스터 간 거리가 실제 유사도를 정확히 반영하는 것은 아닐 수 있음)

  특히 `n_neighbors` 값이 작을 때, 랜덤 노이즈가 가짜 클러스터처럼 보일 수 있으므로 신중한 해석이 필요

  $\hookrightarrow$ 하나의 시각화 결과만으로 판단하지 않고, 여러 파라미터로 다양한 시각화를 시도하는 것이 중요
